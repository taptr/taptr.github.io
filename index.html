<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Visual Instruction Tuning">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TAPTR: Tracking Any Point TRansformers</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="images/icon-cl.png"> -->
  <!-- <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet"> -->


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.39.0/gradio.js"></script>
  
<script id="MathJax-script" async
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">TAPTR: <u>T</u>rack <u>A</u>ny <u>P</u>oint <u>TR</u>ansformers
            <h4></h4>
            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Contributors:
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=zdgHNmkAAAAJ&hl=en" style="color:#f68946;font-weight:normal;">Hongyang Li<sup><b style="color:#f68946; font-weight:normal">&#x25B6 </b></sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=-RSeOl0AAAAJ&hl=en&oi=ao" style="color:#f5643f;font-weight:normal;">Jinyuan Qu<sup><b style="color:#f5643f; font-weight:normal">&#x25B6 </b></sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&hl=zh-CN" style="color:#a800d7;font-weight:normal;">Hao Zhang<sup><b style="color:#a800d7; font-weight:normal">&#x25B6 </b></sup></a>,
              </span>
              <span class="author-block">
                <a href="https://lsl.zone/" style="color:#f5643f;font-weight:normal;">Shilong Liu<sup><b style="color:#f5643f; font-weight:normal">&#x25B6 </b></sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN&oi=ao" style="color:#0071f2;font-weight:normal;">Zhaoyang Zeng<sup><b style="color:#0071f2; font-weight:normal">&#x25B6 </b></sup></a>,
              </span>
              <span class="author-block">
                <a href="https://rentainhe.github.io/" style="color:#0071f2;font-weight:normal;">Tianhe Ren<sup><b style="color:#0071f2; font-weight:normal">&#x25B6 </b></sup></a>,
              </span>
              <span class="author-block">
                <a href="https://fengli-ust.github.io/" style="color:#a800d7;font-weight:normal;">Feng Li<sup><b style="color:#a800d7; font-weight:normal">&#x25B6 </b></sup></a>,
              </span>
              <span class="author-block">
                <a href="https://www.leizhang.org/" style="color:#0071f2;font-weight:normal;">Lei Zhang<sup>&#x2628;</sup><sup><b style="color:#0071f2; font-weight:normal">&#x25B6 </b></sup></a>,
              </span>
              
            </div>
            
            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#0071f2; font-weight:normal">&#x25B6 </b> IDEA Research</span>
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> SCUT</b></span>
              <span class="author-block"><b style="color:#f5643f; font-weight:normal">&#x25B6 </b> Tsinghua University </span>
              <span class="author-block"><b style="color:#a800d7; font-weight:normal">&#x25B6 </b> HKUST</span>
            </div>
            
            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>&#x2628;</sup> Corresponding author</span><br>
              <span class="author-block">This work was done while Hongyang Li, Jinyuan Qu, Hao Zhang, Shilong Liu, and Feng Li were interns at IDEA.</span>
              
            </div>
            <br>


            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2403.13042" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>TAPTRv1 ECCV'24</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2407.16291" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>TAPTRv2 NeurIPS'24</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="todo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>TAPTRv3 ArXiv'24</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/IDEA-Research/TAPTR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://taptr-trajectory.deepdataspace.com" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Demo-Trajectory</span>
                  </a>
                </span> -->
                <!-- <span class="link-block">
                  <a href="https://taptr-videoediting.deepdataspace.com" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Demo-TrackAnyArea</span>
                  </a>
                </span> -->

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section"  style="background-color:#efeff081">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"><span style="font-size: 100%;">üõ†Ô∏è Application of TAPTRs in video editing and trajectory prediction. </h2>
          <div class="video">
            <video width="320" height="240" controls>
              <source src="./images/VideoEdit_box.mp4" type="video/mp4">
            </video>
            <video width="320" height="240" controls>
              <source src="./images/VideoEdit_sofa.mp4" type="video/mp4">
            </video>
            <video width="320" height="240" controls>
              <source src="./images/VideoEdit_bed.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video">
            <video width="320" height="240" controls>
              <source src="./images/VideoEdit_roll_basketball.mp4" type="video/mp4">
            </video>
            <video width="320" height="240" controls>
              <source src="./images/Traj_Handwriting.mp4" type="video/mp4">
            </video>
            <video width="320" height="240" controls>
              <source src="./images/Traj_RabbitAndYogurt.mp4" type="video/mp4">
            </video>
          </div>
          Note: For the video editing demos, we plot "TAPTR" on the first frame of the video, then we sample points in the plot area densely and track them throughout the video to achieve video editing.
  </div>
  <br>
  </section>
  
  

  <section class="section"  style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> üß¨ What is TAPTR.</h2>
          <div class="content has-text-justified">
            <p>
              Inspired by recent visual prompt-based detection [1], we propose to convert Track Any Point (TAP) task to point-level visual prompt detection task. Building upon the recent advanced DEtection TRansformer (DETR) [2, 3, 4, 5], we propose our Track Any Point TRansformer (TAPTR).
           </p>
           <p>
            [1] T-rex2: Towards Generic Object Detection via Text-visual Prompt Synergy. IDEA-Research. ECCV2024.
           </p>
           <p>
            [2] DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. IDEA-Research. ICLR2022.
           </p>
           <p>
            [3] DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. IDEA-Research. CVPR2022.
           </p>
           <p>
            [4] DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. IDEA-Research. ICLR2023.
           </p>
           <p>
            [5] DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding. IDEA-Research. ArXiv2024.
          </div>
        </div>
      </div>
        
    </div>
  </section>





<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">üë£ From V1 to V3, a brief overview.</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 

        <h5 class="title is-4">TAPTRv1 - Simple yet strong baseline.</h5>

        <p>
          TAPTRv1 first proposes to address TAP task <strong>from the perspective of detection</strong>. Instead of building upon the traditional optical flow methods, TAPTRv1 is also the first to propose adopting the more advanced <strong>DETR-like framework</strong> for the TAP task. Compared with previous methods, TAPTRv1 has a <strong>clearer and simpler definition of point query and better performance</strong>.
        </p>
        <p>
          <i>Although TAPTRv1 achieves SoTA performance with the DETR-like framework, TAPTRv1 still needs the source-consuming cost-volume to obtain its optimal performance.</i>
        </p>
          <centering>
            <div style="text-align: center;">
              <img id="teaser" width="90%" src="images/TAPTRv1.png" style="margin-bottom: 30px;">     
            </div>
          </centering> 


        
        <h5 class="title is-4">TAPTRv2 - Simpler and stronger.</h5>
      
        <p>
          TAPTRv2 finds that the reliance of cost-volume stems from the <strong>domain gap between training and evaluation data</strong>. The use of cost-volume in TAPTRv1 will also <strong>contaminate the point query</strong>. TAPTRv2 finds that the <strong>attention weights and cost-volume are essentially the same</strong> (attention weights in key-aware deformable cross attention can be regarded as sparse cost-volume). TAPTRv2 proposes the <strong>Attention-based Position Update (APU)</strong> to utilize this domain-agnostic information while preventing point queries from being contaminated. The elimination of cost-volume makes TAPTRv2 <strong>unifies</strong> the framework of both object-level and point-level perception.
        </p>
        <p>
          <i>Although TAPTRv2 achieves simpler framework and better performance, TAPTRv2‚Äôs performance in long videos is still not satisfying.</i>
        </p>
          <centering>
            <div style="text-align: center;">
              <img id="teaser" width="90%" src="images/TAPTRv2.png" style="margin-bottom: 30px;">     
            </div>
          </centering> 


        <h5 class="title is-4">TAPTRv3 - Much stronger, especially on long-term tracking.</h5>
    
        <p>
          TAPTRv3 finds that the poor performance of TAPTRv2 on long videos is due to its shortage of feature querying in both spatial and temporal dimensions in long videos. For <strong>better temporal feature querying</strong>, instead of utilizing the RNN-like long-temporal modeling, TAPTRv3 extends the temporal attention from a small window to arbitrary length while considering the target tracking points' visibility, and proposes the <strong>Visibility-aware Long-Temporal Attention</strong> (VLTA). For <strong>better spatial feature querying</strong>, TAPTRv3 utilizes the spatial context to improve the quality of attention weights in cross attention, and proposes the <strong>Context-aware Cross Attention</strong> (CCA). To help TAPTRv3 reestablish tracking when a scene cut occurs with sudden large motion, which is quite prevalent in long-term videos, TAPTRv3 proposes to <strong>trigger global matching module to reset point queries' initial locations when a scene cut is detected</strong>. <strong>TAPTRv3 achieves SoTA performance on almost all TAP datasets, even when compared with methods trained on large-scale extra internal data, TAPTRv3 is still competitive.</strong>
        </p>
          <centering>
            <div style="text-align: center;">
              <img id="teaser" width="70%" src="images/TAPTRv3.png" style="margin-bottom: 30px;">     
            </div>
          </centering> 
      </div>
    </div>
  </div>
</div>
</section>

<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">üíØ Performance
      </h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 

          <centering>
            <div style="text-align: center;">
              <img id="teaser" width="90%" src="images/performance.png">     
            </div>
          </centering> 
        <p>
          
          Through three versions of optimization, TAPTRv3 achieves state-of-the-art performance. Even when compared with methods trained on large-scale extra internal data (we only train TAPTRv3 on a small synthetic dataset, while BootsTAPIR and CoTracker3 are refined on their large-scale internal real-world dataset), TAPTRv3 is still competitive.

        </p>

      </div>
    </div>
  </div>
</div>
</section>


<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">üóì Future work and collaboration opportunities.
      </h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 
        <p>
          1. Extend TAPTR to Track Any Visual Prompt TRansformer (TAVTR).
        </p>
        <p>
          2. Exploring the application of TAPTR in embodied AI scenarios.
        </p>
        <p>
          If you have any interest in collaboration, feel free to contact us!
        </p>

      </div>
    </div>
  </div>
</div>
</section>


<script>
let slideIndex = 0;
showSlides();

function showSlides() {
  let i;
  let slides = document.getElementsByClassName("mySlides");
  let dots = document.getElementsByClassName("dot");
  for (i = 0; i < slides.length; i++) {
    slides[i].style.display = "none";  
  }
  slideIndex++;
  if (slideIndex > slides.length) {slideIndex = 1}    
  for (i = 0; i < dots.length; i++) {
    dots[i].className = dots[i].className.replace(" active", "");
  }
  slides[slideIndex-1].style.display = "block";  
  dots[slideIndex-1].className += " active";
  setTimeout(showSlides, 4000); // Change image every 1 seconds
}
</script>



  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{li2024taptr,
          title={{TAPTR: Tracking Any Point with Transformers as Detection}},
          author={Li, Hongyang and Zhang, Hao and Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Lei},
          booktitle={European Conference on Computer Vision},
          pages={57--75},
          year={2024},
          organization={Springer}
        }
        @article{li2024taptrv2
          title={{TAPTRv2: Attention-based Position Update Improves Tracking Any Point}},
          author={Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Feng Li, Tianhe Ren, Bohan Li, Lei Zhang},
          journal={Advances in Neural Information Processing Systems},
          year={2024}
        }
        @article{qu2024taptrv3,
          title={{TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video}},
          author={Jinyuan Qu, Hongyang Li, Shilong Liu, Tianhe Ren, Zhaoyang Zeng, Lei Zhang},
          journal={arXiv preprint},
          year={2024}
        }
  </code></pre>
    </div>
  </section>
  

</body>

</html>
